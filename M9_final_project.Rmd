---
title: "BIT 4624 - Final Exam"
output: html_document
date: '2022-07-30'
---Matthias Cannon

# Final Exam: Exploratory Analysis of Vulnerabilities 

For the final exam we will be exploring a data set of the vulnerabilities 
published and reserved in 2021.  

Your goals in the final exam are:

1) build your intuition about the data
  1a) explore each column of data, describe what the variables mean, identify any 
      missing data and discuss anything you find interesting about the variable.
  1b) explore the covariance between interesting pairs of variables, discuss
      anything you find interesting about the interactions
  1c) identify any possible research questions (no matter how silly) for each 
      aspect you explore.  
  
2) Develop two different research questions about the data. For each question:
  2a) Discuss why the question is interesting and worth answering.
  2b) Produce a visual that you think best answers the question.
  2c) Discuss the outcome, think about what may be driving the results you have,
      Discuss what advice you would give or what the results may mean for 
      security practitioners



```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```


# 1) Data Load

Grab the data associated with Module 9, bring it locally and load it up
in this next block. Call the variable "vulns".

user selects data from computer file
```{r}
vulns <- read_csv(file.choose())
```

# 2. Build your intuition

Explore each column of data, describe what the variables mean (see the description 
of the data in the module). Identify any missing data and discuss anything you 
find interesting about the variable. Identify any possible research questions 
(no matter how silly) for everything you explore.

Make a plot for each variable, use a different block of code for each, and 
write a sentence or two discussing each one, the range, missing data, importance,
and the relevance. Be sure to explore possible research questions for each. 

```{r}
# Missing values
vulns %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  gather() %>% 
  mutate(percent =round(value/nrow(vulns)*100,2))

vulns <- na.omit(vulns)

# descriptive statistics
summary(vulns)
glimpse(vulns)

vulns$published <- as.Date(vulns$published, format="%m/%d/%Y")

```
It appears that about a third of the data is missing over almost all columns in the data. These observations are useless for our evaluation of the data so I've dropped them from the data. It might be interesting to look deeper into how base score has changed over time so I changed `published` to a date data type for plotting later. 

Average base score is 7.091, which seems somewhat high.

## Plots 

Not doing a plot for cve or published because each unique cve observation is unique and a plot of published date would be useless. 

###state
```{r}
ggplot(vulns, aes(x=state))+geom_bar(fill = "steelblue")

vulns$state[vulns$state == "disputed"]
```
only 63 disputed entries from the 20,025 observations, all remaining entries are published.

###Base score
```{r}
ggplot(vulns, aes(x=cvss3.base_score))+geom_histogram(fill = "steelblue", bins = 20)

vulns %>% 
  filter(cvss3.base_score > 6.0)
```
Interestingly over 10% of the data has a severity rating of 10 and it appears that more than half of the data has a severity rating more than 6 from the histogram. To test this I used a pipe operator and filter function and found that this was true, 14,507 rows of the 20,025 had a score higher than 6. It may be interesting to plot how the distribution of base score changes for different values of a categorical variables such as user_interaction, priv_req, attack_complexity or scope.

### attack_vector
```{r}
ggplot(vulns, aes(x=cvss3.attack_vector))+geom_bar(fill = "steelblue")
```
More than half of the attack vector observations are network and a very low amount belong to adjacent_network and physical.

###attack_complexity
```{r}
ggplot(vulns, aes(x=cvss3.attack_complexity))+geom_bar(fill = "steelblue")
```
Despite a high amount of high severity base scores there is alarmingly a low amount of high complexity attacks.

### priv_req
```{r}
ggplot(vulns, aes(x=cvss3.priv_req))+geom_bar(fill = "steelblue")
```
Interestingly more than half of the priv_req don't have a classification. We probably won't get too much information by diving deeper into this varialbe. 

### user_interaction
```{r}
ggplot(vulns, aes(x=cvss3.user_interaction))+geom_bar(fill = "steelblue")
```
Could be interesting to see how the requirement of user interaction could effect the severity of a cyber attack.

### scope
```{r}
ggplot(vulns, aes(x=cvss3.scope))+geom_bar(fill = "steelblue")
```

### confidentiality_impact
```{r}
ggplot(vulns, aes(x=cvss3.confidentiality_impact))+geom_bar(fill = "steelblue")
```
As alarming as it sounds that more than half of observations have high confidentiality_impact, it is not surprising considering my earlier findings concerning severity of attacks.

### integrity_impact
```{r}
ggplot(vulns, aes(x=cvss3.integrity_impact))+geom_bar(fill = "steelblue")
```
Integrity impact has a very similar distribution to confidentiality impact.

### availability impact
```{r}
ggplot(vulns, aes(x=cvss3.availability_impact))+geom_bar(fill = "steelblue")
```
Low availability impact is more common than a low integrity or low confidentiality impact.

### score
```{r}
ggplot(vulns, aes(x=epss.score))+geom_histogram(fill = "steelblue", bins = 10)
```
epps.score is very low more than 90% of the time with values reaching up to blank.

### percentile
```{r}
ggplot(vulns, aes(x=epss.percentile))+geom_histogram(fill = "steelblue", bins = 10)
```
As described on canvas this variable corrects for the skewness in the score variable previously mentioned. This variable's distribution more closely resembles the distribution of base score. 

### github
```{r}
ggplot(vulns, aes(x=exp_github))+geom_bar(fill = "steelblue")
```
Most entries do not have code attached to github.

### metsploit
```{r}
ggplot(vulns, aes(x=exp_metasploit))+geom_bar(fill = "steelblue")
```
Very few observations include metasploit

### exploitdb
```{r}
ggplot(vulns, aes(x=exp_exploitdb))+geom_bar(fill = "steelblue")
```


# 3. Explore pairs of variables 

Explore different pairs of data that seem interesting. Discuss anything you 
find interesting about the interaction. Identify any possible research questions 
(no matter how silly) for everything you explore.

Pick out several variables and explore how the two variables overlap or interact.
Make a plot for each pair, use a different block of code for each, and 
write a sentence or two discussing each pair: the trends the data shows, importance,
and the relevance. Be sure to explore possible research questions for each. 

##Correlation plot
```{r}
library(corrplot)

Corr_data <- vulns %>% 
  select(c(cvss3.base_score,epss.score,epss.percentile))

corr_col_names <- c("Base_score","EPSS_Score","EPSS_percentile")

names(Corr_data) <- corr_col_names

correlation<-round(cor(Corr_data),4)

corrplot(correlation)
```
## Two variable analysis

### base score and attack complexity
```{r}
ggplot(vulns, aes(x=cvss3.base_score, fill = cvss3.attack_complexity)) +geom_density(color = NA, alpha = 2/3)
```
I was suprised to find that base score had a higher volume of observations between scores 9 and 10 for low complexity attacks when compared to high complexity. 

## Base score and user interaction
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.user_interaction, fill = cvss3.user_interaction))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```
As I expected requiring user interaction decreases the severity score on average. Only 10.0 rated attacks occurred when there was no user interaction. This would insinuate that user interaction is a key determinant in quality security.

# 4. Research Questions

State your research question. 

Discuss why you think it's a good research question and why the question is
worth answering (what action or reaction would the answer drive?). 

Produce one or more visuals to support and answer this research question. 

Write up the outcome and discuss why you made the decisions you did while generating
the plot, Discuss the results and what may be driving those results. Discuss what 
advice the outcome may generate and why security practitioners may find it interesting. 

I would be surprised if you can explain all the above in anything less than a paragraph, most likely two paragraphcs. 

# 4a. Research Question 1

## Is there a trend in scores over time?

Here is my analysis and a visual answering the question:

```{r}
ggplot(vulns, aes(x=published, y = cvss3.base_score))+geom_line(color = "steelblue")
```
```{r}
ggplot(vulns, aes(x=published, y = epss.percentile))+geom_line(color = "steelblue")
```

Here is my discussion of the research question and outcome:

I thought this would be a compelling research question since a trend overtime could help identify if severity of attacks is decreasing, increasing or staying the same. If decreasing we may be interested in looking into the impact new security features have had on severity of attacks. However, it appears from our plots that there hasn't been much of a trend at all over the past 18 months. The only difference that stood out is the change in breadth over time. The breadth of base scores and percentile scores has become more narrow from April to July this year. Lower base scores have become less common over this time period and higher percentiles have become less common. 

Our evidence give conflicting results for our research question since lack of lower base scores would suggest worse security and lack of higher percentiles would suggest forecasts that predicting high probability of exploitation as often. 

# 4b. Research Question 2

## What is the most crucial component to severity of base score?

Here is my analysis and a visual answering the question:

### user interaction
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.user_interaction, fill = cvss3.user_interaction))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```

### attack vector
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.attack_vector, fill = cvss3.attack_vector))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```
As mentioned in the documentation for cvss there is a higher risk of severity for base score if a greater number of hackers can reach a server (network or adjacent network). However this isn't always something that can within control of the data holder. 

### priv req
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.priv_req, fill = cvss3.priv_req))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```
High privileges appears to sometimes have an effect when compared to low, but low has a significant impact when compared to none.

### User Interaction
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.user_interaction, fill = cvss3.user_interaction))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```
User interaction appears to still be one of the most impactful factor thus far. 

### Scope
```{r}
ggplot(vulns, aes(x=cvss3.base_score, y = cvss3.scope, fill = cvss3.scope))+geom_violin(color=NA, alpha=2/3) + geom_boxplot(width=0.4,fill=NA,color="black")
```
Changed scope appears to have a large impact on score, however, this may be due to the fact that the vulnerable and impacted scope are different when scope is changed.

Not going to plot any more variables since the remaining variables are reactive variables based on severity of attack and not preventative measures.

Here is my discussion of the research question and outcome:

After evaluating base score over all preventative measure categorical variables I would recommend that the first step to take to prevent high severity attacks would be implementing low privilege requirements. The first reason for this is it would likely take less resources than implementing user interaction. Secondly it also appears to be the most impactful on score on average. Low privilege requirements decreased median base score by roughly 2.5 points compared to a decrease of about 1.3 for user interaction. Changed vs. unchanged scope also had similar results to user interaction making it a less than ideal option as well.





